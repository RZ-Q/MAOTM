# --- RODE specific parameters ---

# use epsilon greedy action/role selector
action_selector: "soft_epsilon_greedy"
action_encoder: "obs_reward"
epsilon_start: 1.0
epsilon_finish: 0.05
role_epsilon_finish: 0.05
epsilon_anneal_time: 70000
epsilon_anneal_time_exp: 70000

runner: "episode"

buffer_size: 5000

# update the target network every {} episodes
target_update_interval: 200

# use the Q_Learner to train
agent_output_type: "q"
learner: "EWM_learner"
double_q: True
mixer: "qmix"
role_mixer: "qmix"
mixing_embed_dim: 32
hypernet_layers: 2
hypernet_embed: 64

name: "EWM"
mac: "rode_mac"
agent: "rode"
role: 'dot'
role_selector: 'dot'
bi_opt: False

n_role_clusters: 3
role_interval: 5
state_latent_dim: 32
action_latent_dim: 20
role_action_spaces_update_start: 500

# for interpretability
verbose: False

# for world model
model_type: "rtgs_obs_roles_actions_reward"
block_size: 4
context_length: 1
max_timestep: 400
n_layer: 2
n_head: 2
n_embd: 32
embd_pdrop: 0.
resid_pdrop: 0.
attn_pdrop: 0.
weight_decay: 0.1
betas: [0.9, 0.95]